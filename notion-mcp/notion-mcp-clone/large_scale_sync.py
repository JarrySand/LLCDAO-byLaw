#!/usr/bin/env python3
"""
å¤§è¦æ¨¡DAOå”åƒã‚·ã‚¹ãƒ†ãƒ  - æ•°ç™¾å˜ä½åŒæœŸå¯¾å¿œ
Cursor + Notion MCP å®Œå…¨è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
from notion_mcp.client import NotionClient
import yaml
import time

# æ–°ã—ã„Markdownå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from enhanced_markdown_converter import EnhancedMarkdownConverter

class LargeScaleSyncSystem:
    """æ•°ç™¾å˜ä½ã®åŒæœŸã«å¯¾å¿œã—ãŸå¤§è¦æ¨¡åŒæœŸã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, api_key: str, project_root: str = "C:/Users/zukas/mybrain"):
        self.client = NotionClient(api_key)
        self.project_root = Path(project_root)
        self.database_id = "24039d01-e781-8040-8205-d82e13a1e8f0"
        self.sync_log = self.project_root / ".cursor" / "sync_log.json"
        self.batch_size = 10  # APIåˆ¶é™å¯¾å¿œ
        self.retry_count = 3
        
        # æ–°ã—ã„Markdownå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        self.markdown_converter = EnhancedMarkdownConverter()
        
    async def sync_markdown_files_to_notion(self, folder_path: str) -> Dict[str, Any]:
        """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬NotionåŒæœŸ"""
        print(f"ğŸš€ å¤§è¦æ¨¡åŒæœŸé–‹å§‹: {folder_path}")
        
        folder = Path(folder_path)
        markdown_files = list(folder.rglob("*.md"))
        
        results = {
            "total_files": len(markdown_files),
            "processed": 0,
            "created": 0,
            "updated": 0,
            "errors": [],
            "start_time": datetime.now().isoformat()
        }
        
        # ãƒãƒƒãƒå‡¦ç†ã§å®Ÿè¡Œ
        for i in range(0, len(markdown_files), self.batch_size):
            batch = markdown_files[i:i + self.batch_size]
            print(f"ğŸ“¦ ãƒãƒƒãƒ {i//self.batch_size + 1}: {len(batch)}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­...")
            
            batch_tasks = []
            for md_file in batch:
                batch_tasks.append(self._process_markdown_file(md_file, results))
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # APIåˆ¶é™å›é¿ã®ãŸã‚ã®å¾…æ©Ÿ
            await asyncio.sleep(1)
            
        results["end_time"] = datetime.now().isoformat()
        await self._save_sync_log(results)
        
        print(f"âœ… åŒæœŸå®Œäº†: {results['created']}ä»¶ä½œæˆ, {results['updated']}ä»¶æ›´æ–°")
        return results
    
    async def _process_markdown_file(self, md_file: Path, results: Dict[str, Any]):
        """å€‹åˆ¥Markdownãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆãƒ•ãƒ«åŒæœŸå¯¾å¿œï¼‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿
            content = md_file.read_text(encoding='utf-8')
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = self._extract_metadata(content, md_file)
            
            # Notionå½¢å¼ã«å¤‰æ›
            properties = self._convert_to_notion_properties(metadata)
            all_blocks = self._convert_content_to_blocks_full(content)
            
            # æ—¢å­˜ãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨é‡è¤‡å¯¾å¿œï¼‰
            existing_pages = await self._find_all_existing_pages(metadata["title"])
            
            if existing_pages:
                # å…¨é‡è¤‡ãƒšãƒ¼ã‚¸ã‚’å‰Šé™¤ãƒ»å†ä½œæˆæ–¹å¼ï¼ˆç¢ºå®Ÿãªæ›´æ–°ï¼‰
                print(f"ğŸ”„ {len(existing_pages)}å€‹ã®é‡è¤‡ãƒšãƒ¼ã‚¸ç™ºè¦‹ãƒ»å‰Šé™¤ä¸­: {metadata['title']}")
                for page in existing_pages:
                    await self._delete_page_with_retry(page["id"])
                results["updated"] += 1
                
            # æ–°è¦ãƒšãƒ¼ã‚¸ä½œæˆï¼ˆæœ€åˆã®50ãƒ–ãƒ­ãƒƒã‚¯ - ã‚ˆã‚Šå®‰å…¨ãªã‚µã‚¤ã‚ºï¼‰
            initial_blocks = all_blocks[:50] if len(all_blocks) > 50 else all_blocks
            page = await self._create_page_with_retry(properties, initial_blocks)
            page_id = page.id
            
            if not existing_pages:
                results["created"] += 1
                print(f"âœ… ä½œæˆ: {metadata['title']} (åˆæœŸ{len(initial_blocks)}ãƒ–ãƒ­ãƒƒã‚¯)")
            else:
                print(f"ğŸ†• å†ä½œæˆ: {metadata['title']} (åˆæœŸ{len(initial_blocks)}ãƒ–ãƒ­ãƒƒã‚¯)")
            
            # æ®‹ã‚Šã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ®µéšçš„è¿½åŠ 
            if len(all_blocks) > 50:
                await self._append_remaining_blocks(page_id, all_blocks[50:], metadata["title"])
            
            # processed ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’é©åˆ‡ã«å‡¦ç†
            if "processed" not in results:
                results["processed"] = 0
            results["processed"] += 1
            print(f"ğŸ“š ãƒ•ãƒ«åŒæœŸå®Œäº†: {metadata['title']} ({len(all_blocks)}ãƒ–ãƒ­ãƒƒã‚¯)")
            
        except Exception as e:
            error_info = {
                "file": str(md_file),
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            results["errors"].append(error_info)
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {md_file.name} - {e}")
    
    async def _append_remaining_blocks(self, page_id: str, remaining_blocks: List[Dict[str, Any]], title: str):
        """æ®‹ã‚Šã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ®µéšçš„ã«è¿½åŠ ï¼ˆé©å¿œçš„ã‚µã‚¤ã‚ºèª¿æ•´ï¼‰"""
        max_blocks_per_batch = 50  # ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º
        total_remaining = len(remaining_blocks)
        batch_count = (total_remaining + max_blocks_per_batch - 1) // max_blocks_per_batch
        
        print(f"  ğŸ”„ æ®µéšçš„ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ : {total_remaining}ãƒ–ãƒ­ãƒƒã‚¯ â†’ {batch_count}å›ã«åˆ†å‰²")
        
        for batch_num in range(batch_count):
            start_idx = batch_num * max_blocks_per_batch
            end_idx = min(start_idx + max_blocks_per_batch, total_remaining)
            batch_blocks = remaining_blocks[start_idx:end_idx]
            
            # é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒªãƒˆãƒ©ã‚¤
            success = await self._append_batch_with_adaptive_retry(page_id, batch_blocks, batch_num + 1, batch_count)
            
            if success:
                print(f"  âœ… ãƒãƒƒãƒ {batch_num + 1}/{batch_count}: {len(batch_blocks)}ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ ")
            else:
                print(f"  âš ï¸ ãƒãƒƒãƒ {batch_num + 1}/{batch_count}: ä¸€éƒ¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            # APIåˆ¶é™å¯¾å¿œã®å¾…æ©Ÿ
            await asyncio.sleep(1.5)  # å°‘ã—é•·ã‚ã®å¾…æ©Ÿ
    
    async def _append_batch_with_adaptive_retry(self, page_id: str, blocks: List[Dict[str, Any]], batch_num: int, total_batches: int) -> bool:
        """é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ ãƒªãƒˆãƒ©ã‚¤"""
        if not blocks:
            return True
        
        # æœ€åˆã¯é€šå¸¸ã®ãƒªãƒˆãƒ©ã‚¤ã‚’è©¦è¡Œ
        try:
            await self._append_blocks_with_retry(page_id, blocks)
            return True
        except Exception as e:
            error_msg = str(e).lower()
            
            # ã‚µã‚¤ã‚ºé–¢é€£ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯åˆ†å‰²ã—ã¦å†è©¦è¡Œ
            if any(keyword in error_msg for keyword in ['400', 'bad request', 'too large', 'size', 'limit']):
                print(f"    ğŸ“¦ ãƒãƒƒãƒ{batch_num} ã‚µã‚¤ã‚ºã‚¨ãƒ©ãƒ¼æ¤œå‡º â†’ åˆ†å‰²å‡¦ç†é–‹å§‹")
                return await self._split_and_retry_batch(page_id, blocks, batch_num)
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å¾“æ¥é€šã‚Šã‚¹ã‚­ãƒƒãƒ—
                print(f"    âŒ ãƒãƒƒãƒ{batch_num} å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆéã‚µã‚¤ã‚ºé–¢é€£ï¼‰: {e}")
                return False
    
    async def _split_and_retry_batch(self, page_id: str, blocks: List[Dict[str, Any]], batch_num: int) -> bool:
        """ãƒãƒƒãƒã‚’åˆ†å‰²ã—ã¦å†å¸°çš„ã«ãƒªãƒˆãƒ©ã‚¤"""
        total_blocks = len(blocks)
        
        # 1ãƒ–ãƒ­ãƒƒã‚¯ã¾ã§åˆ†å‰²ã—ãŸå ´åˆã®æœ€çµ‚å‡¦ç†
        if total_blocks == 1:
            try:
                await self._append_blocks_with_retry(page_id, blocks)
                return True
            except Exception as e:
                print(f"      âŒ å˜ä¸€ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†å¤±æ•—: {e}")
                return False
        
        # ãƒãƒƒãƒã‚’åŠåˆ†ã«åˆ†å‰²
        mid = total_blocks // 2
        first_half = blocks[:mid]
        second_half = blocks[mid:]
        
        print(f"      ğŸ”„ ãƒãƒƒãƒ{batch_num} åˆ†å‰²: {total_blocks} â†’ {len(first_half)} + {len(second_half)}")
        
        # å‰åŠãƒ»å¾ŒåŠã‚’é †æ¬¡å‡¦ç†
        success_count = 0
        
        for i, sub_batch in enumerate([first_half, second_half], 1):
            try:
                await self._append_blocks_with_retry(page_id, sub_batch)
                print(f"        âœ… åˆ†å‰²{i}: {len(sub_batch)}ãƒ–ãƒ­ãƒƒã‚¯æˆåŠŸ")
                success_count += 1
            except Exception as e:
                # ã•ã‚‰ã«åˆ†å‰²ãŒå¿…è¦ãªå ´åˆã¯å†å¸°å‡¦ç†
                if len(sub_batch) > 1:
                    sub_success = await self._split_and_retry_batch(page_id, sub_batch, f"{batch_num}.{i}")
                    if sub_success:
                        success_count += 1
                else:
                    print(f"        âŒ åˆ†å‰²{i} å˜ä¸€ãƒ–ãƒ­ãƒƒã‚¯å¤±æ•—: {e}")
            
            # å„åˆ†å‰²ã®é–“ã«çŸ­ã„å¾…æ©Ÿ
            await asyncio.sleep(0.5)
        
        return success_count > 0  # ä¸€éƒ¨ã§ã‚‚æˆåŠŸã™ã‚Œã°True
    
    async def _append_blocks_with_retry(self, page_id: str, blocks: List[Dict[str, Any]]):
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ–ãƒ­ãƒƒã‚¯è¿½åŠ """
        for attempt in range(self.retry_count):
            try:
                return await self.client.append_block_children(page_id, blocks)
            except Exception as e:
                if attempt == self.retry_count - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
    
    async def _create_page_with_retry(self, properties: Dict[str, Any], children: List[Dict[str, Any]]):
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒšãƒ¼ã‚¸ä½œæˆ"""
        for attempt in range(self.retry_count):
            try:
                return await self.client.create_page(
                    parent_id=self.database_id,
                    properties=properties,
                    children=children
                )
            except Exception as e:
                if attempt == self.retry_count - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
    
    def _extract_metadata(self, content: str, file_path: Path) -> Dict[str, Any]:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        lines = content.split('\n')
        
        # YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼æ¤œå‡º
        if lines[0].strip() == '---':
            yaml_end = None
            for i, line in enumerate(lines[1:], 1):
                if line.strip() == '---':
                    yaml_end = i
                    break
            
            if yaml_end:
                try:
                    yaml_content = '\n'.join(lines[1:yaml_end])
                    metadata = yaml.safe_load(yaml_content)
                    if isinstance(metadata, dict):
                        return self._normalize_metadata(metadata, file_path)
                except:
                    pass
        
        # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        title = lines[0].replace('#', '').strip() if lines else file_path.stem
        
        return {
            "title": title,
            "category": self._infer_category(file_path),
            "status": "æœªç€æ‰‹",
            "priority": "ä¸­",
            "file_path": str(file_path),
            "last_modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        }
    
    def _normalize_metadata(self, metadata: Dict[str, Any], file_path: Path) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–"""
        return {
            "title": metadata.get("title", file_path.stem),
            "category": metadata.get("category", self._infer_category(file_path)),
            "status": metadata.get("status", "æœªç€æ‰‹"),
            "priority": metadata.get("priority", "ä¸­"),
            "due_date": metadata.get("due_date"),
            "tags": metadata.get("tags", []),
            "file_path": str(file_path),
            "last_modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        }
    
    def _infer_category(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæ¨å®šï¼ˆLLCDAOå°‚ç”¨ï¼‰"""
        path_str = str(file_path).lower()
        
        if "token" in path_str or "ãƒˆãƒ¼ã‚¯ãƒ³" in path_str or "week1" in path_str:
            return "ãƒˆãƒ¼ã‚¯ãƒ³è¦ç¨‹"
        elif "assembly" in path_str or "ç·ä¼š" in path_str or "week2" in path_str:
            return "ç·ä¼šè¦ç¨‹"
        elif "operation" in path_str or "é‹å–¶è¦ç¨‹" in path_str or "week3" in path_str:
            return "é‹å–¶è¦ç¨‹"
        elif "treasury" in path_str or "ãƒˆãƒ¬ã‚¸ãƒ£ãƒªãƒ¼" in path_str or "week4" in path_str:
            return "ãƒˆãƒ¬ã‚¸ãƒ£ãƒªãƒ¼è¦ç¨‹"
        elif "charter" in path_str or "æ†²ç« " in path_str or "week5" in path_str:
            return "DAOæ†²ç« "
        elif "integration" in path_str or "çµ±åˆ" in path_str or "week6" in path_str:
            return "çµ±åˆ"
        else:
            return "ãã®ä»–"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _convert_to_notion_properties(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Notionãƒ—ãƒ­ãƒ‘ãƒ†ã‚£å½¢å¼ã«å¤‰æ›"""
        properties = {
            "åå‰": {"title": [{"text": {"content": metadata["title"]}}]},
            "Category": {"select": {"name": metadata["category"]}},
            "Status": {"status": {"name": metadata["status"]}},
            "Priority": {"select": {"name": metadata["priority"]}}
        }
        
        if metadata.get("due_date"):
            properties["Due Date"] = {"date": {"start": metadata["due_date"]}}
            
        if metadata.get("tags"):
            # Tagsãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«è¿½åŠ ï¼ˆè¤‡æ•°é¸æŠï¼‰
            tag_options = [{"name": tag} for tag in metadata["tags"]]
            properties["Tags"] = {"multi_select": tag_options}
            
        return properties
    
    def _convert_content_to_blocks(self, content: str) -> List[Dict[str, Any]]:
        """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’notionãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã«å¤‰æ›ï¼ˆ100ãƒ–ãƒ­ãƒƒã‚¯åˆ¶é™å¯¾å¿œï¼‰"""
        print("ğŸ”„ é«˜åº¦ãªMarkdownå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ä¸­ï¼ˆåˆ¶é™ç‰ˆï¼‰...")
        
        # æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨
        all_blocks = self.markdown_converter.convert_content_to_blocks(content)
        
        # ãƒ–ãƒ­ãƒƒã‚¯åˆ¶é™å¯¾å¿œ
        max_blocks = 50  # ã‚ˆã‚Šå®‰å…¨ãªã‚µã‚¤ã‚º
        
        if len(all_blocks) <= max_blocks:
            print(f"âœ… å¤‰æ›å®Œäº†: {len(all_blocks)}ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆï¼ˆåˆ¶é™å†…ï¼‰")
            return all_blocks
        else:
            # åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            limited_blocks = all_blocks[:max_blocks-2]
            limited_blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": "..."}}]}
            })
            limited_blocks.append({
                "object": "block",
                "type": "paragraph", 
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": "ğŸ“„ å®Œå…¨ãªå†…å®¹ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"}}]}
            })
            
            print(f"âš ï¸ ãƒ–ãƒ­ãƒƒã‚¯åˆ¶é™å¯¾å¿œ: {len(all_blocks)} â†’ {len(limited_blocks)}ãƒ–ãƒ­ãƒƒã‚¯")
            print(f"ğŸ“Š å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: å¤ªå­—ãƒ»æ–œä½“ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒªãƒ³ã‚¯ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç­‰")
            
            return limited_blocks
    
    def _convert_content_to_blocks_full(self, content: str) -> List[Dict[str, Any]]:
        """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å®Œå…¨å¤‰æ›ï¼ˆåˆ¶é™ãªã—ï¼‰- æ–°ã—ã„é«˜åº¦ãªå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨"""
        print("ğŸ”„ é«˜åº¦ãªMarkdownå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ä¸­...")
        
        # æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨
        blocks = self.markdown_converter.convert_content_to_blocks(content)
        
        print(f"âœ… å¤‰æ›å®Œäº†: {len(blocks)}ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ")
        print(f"ğŸ“Š å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: å¤ªå­—ãƒ»æ–œä½“ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒªãƒ³ã‚¯ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç­‰")
        
        return blocks
    
    async def _find_all_existing_pages(self, title: str) -> List[Dict[str, Any]]:
        """åŒåã®å…¨æ—¢å­˜ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ï¼ˆé‡è¤‡å¯¾å¿œï¼‰"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®åŒåãƒšãƒ¼ã‚¸ã‚’ã™ã¹ã¦å–å¾—
            results = await self.client.query_database(
                database_id=self.database_id,
                filter={
                    "property": "åå‰",
                    "title": {
                        "equals": title
                    }
                }
            )
            
            if hasattr(results, 'results') and results.results:
                return results.results
            elif isinstance(results, dict) and results.get('results'):
                return results['results']
                
        except Exception as e:
            print(f"  âš ï¸ æ—¢å­˜ãƒšãƒ¼ã‚¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
        return []
    
    async def _update_page_with_retry(self, page_id: str, properties: Dict[str, Any]):
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒšãƒ¼ã‚¸æ›´æ–°"""
        for attempt in range(self.retry_count):
            try:
                return await self.client.update_page(page_id, properties)
            except Exception as e:
                if attempt == self.retry_count - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
    
    async def _delete_page_with_retry(self, page_id: str):
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒšãƒ¼ã‚¸å‰Šé™¤ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰"""
        for attempt in range(self.retry_count):
            try:
                # Notionã§ã¯ãƒšãƒ¼ã‚¸ã‚’å‰Šé™¤ã™ã‚‹ä»£ã‚ã‚Šã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹
                await self.client.update_page(page_id, properties={}, archived=True)
                print(f"    ğŸ—„ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {page_id}")
                return True
            except Exception as e:
                print(f"    âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è©¦è¡Œ {attempt + 1}/{self.retry_count}: {e}")
                if attempt == self.retry_count - 1:
                    print(f"    âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•—: {page_id}")
                    return False
                await asyncio.sleep(2 ** attempt)
    
    async def _save_sync_log(self, results: Dict[str, Any]):
        """åŒæœŸãƒ­ã‚°ä¿å­˜"""
        self.sync_log.parent.mkdir(exist_ok=True)
        
        # æ—¢å­˜ãƒ­ã‚°èª­ã¿è¾¼ã¿
        if self.sync_log.exists():
            with open(self.sync_log, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        else:
            logs = []
        
        # æ–°ã—ã„ãƒ­ã‚°è¿½åŠ 
        logs.append(results)
        
        # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        logs = logs[-100:]
        
        # ä¿å­˜
        with open(self.sync_log, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    api_key = os.getenv("NOTION_API_KEY")
    if not api_key:
        print("âŒ NOTION_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    sync_system = LargeScaleSyncSystem(api_key)
    
    # DAOãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’åŒæœŸ
    dao_folder = "C:/Users/zukas/mybrain/02_Projects/ProjectE_åˆåŒä¼šç¤¾å‹DAO_ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³/Documents"
    
    if Path(dao_folder).exists():
        results = await sync_system.sync_markdown_files_to_notion(dao_folder)
        print(f"\nğŸ“Š åŒæœŸçµæœ:")
        print(f"  ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {results['total_files']}")
        print(f"  âœ… ä½œæˆ: {results['created']}")
        print(f"  ğŸ”„ æ›´æ–°: {results['updated']}")
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {len(results['errors'])}")
    else:
        print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dao_folder}")

if __name__ == "__main__":
    asyncio.run(main())